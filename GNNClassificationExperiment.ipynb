{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cee2724",
   "metadata": {},
   "source": [
    "# GNN Classification of Reeb Graphs from ModelNet10 Point Clouds\n",
    "\n",
    "This notebook reproduces the GNN classification of Decorated Reeb Graphs extracted from ModelNet10 point clouds which is presented in Section 6 of our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeccc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import itertools\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from gtda.plotting import plot_point_cloud\n",
    "from ripser import ripser\n",
    "from persim import PersistenceImager, plot_diagrams\n",
    "import gudhi as gd\n",
    "from ot import fused_gromov_wasserstein2\n",
    "import ot\n",
    "\n",
    "import pickle\n",
    "\n",
    "from DecoratedReebGraphs import *\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ac752",
   "metadata": {},
   "source": [
    "## Loading Point Cloud Data for Visualizations\n",
    "\n",
    "**Note:** You need to unzip the `modelNetPointClouds1024.pickle` file before running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2536741",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/modelNetPointClouds1024.pickle\", \"rb\") as f:\n",
    "    full_dataset = pickle.load(f)\n",
    "    \n",
    "training_data_point_clouds = full_dataset['train_points']\n",
    "testing_data_point_clouds = full_dataset['test_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = full_dataset['train_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels = full_dataset['test_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a8338",
   "metadata": {},
   "source": [
    "# Experiment 1: 1024 Points, Unnormalized, Height Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b2f6e",
   "metadata": {},
   "source": [
    "Load precomputed DRGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/modelNetReebGraphs_1024_UnNormalized_height.pickle\", \"rb\") as f:\n",
    "    full_dataset_Reeb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = full_dataset_Reeb['train_data']\n",
    "testing_data = full_dataset_Reeb['test_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f68cc",
   "metadata": {},
   "source": [
    "### Visualizing the Reeb Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = 1\n",
    "sample_number = 2\n",
    "print('Class:',full_dataset['class_map'][class_id])\n",
    "\n",
    "idx = np.where(full_dataset['test_labels']==class_id)[0][0] + sample_number\n",
    "\n",
    "G = testing_data[idx]\n",
    "\n",
    "plot_3d_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70787496",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_cloud(testing_data_point_clouds[idx,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375c098",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9110bd",
   "metadata": {},
   "source": [
    "This version of node features includes:\n",
    "- persistence statistics\n",
    "- function value\n",
    "- 3d coordinate of the node (averaged position of all real points corresponding to the node)\n",
    "- number of real points corresponding to the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69170e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_feature_matrix(graph,attribute = 'persistence_statistics'):\n",
    "    \n",
    "    return np.array([list(graph.nodes[node][attribute]) + [graph.nodes[node]['function value'],graph.nodes[node]['3d pos'][0],graph.nodes[node]['3d pos'][1],graph.nodes[node]['3d pos'][2],len(graph.nodes[node]['component indices'])] for node in graph.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch_format(graph,label):\n",
    "    \n",
    "    adjacency_matrix = nx.adjacency_matrix(graph)\n",
    "    edge_data = from_scipy_sparse_matrix(adjacency_matrix)\n",
    "    x = get_node_feature_matrix(graph)\n",
    "    y = label\n",
    "    \n",
    "    data = Data(x=torch.Tensor(x), edge_index=edge_data[0], edge_attr = edge_data[1], y=torch.tensor([y],dtype=torch.long))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9d0b8",
   "metadata": {},
   "source": [
    "Setting up data for the GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b27465",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "training_data_list = [convert_to_torch_format(training_data[j],training_labels[j]) for j in range(len(training_data))]\n",
    "train_loader = DataLoader(training_data_list, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "testing_data_list = [convert_to_torch_format(testing_data[j],testing_labels[j]) for j in range(len(testing_data))]\n",
    "test_loader = DataLoader(testing_data_list, batch_size = len(testing_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff858d",
   "metadata": {},
   "source": [
    "## Define Model and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52960742",
   "metadata": {},
   "source": [
    "We will use a simple 4 layer graph convolutional neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6027e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CN3sRVuaQ88l",
    "outputId": "59c998ba-b387-4413-d7d3-2101a27495bd"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "node_feature_size = 35 # Need to change, depending on feature style\n",
    "num_data_classes = 10\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(node_feature_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_data_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65169a15",
   "metadata": {
    "id": "V2Q37tbHyQ6A"
   },
   "source": [
    "Training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe644d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HvhgQoO8Svw4",
    "outputId": "4d17af35-85c8-4bcd-8e16-c514fd2ff714",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = GCN(hidden_channels=256)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "num_epochs = 300\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    test_acc = test(test_loader)\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_model = model\n",
    "    test_accs.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "xs = list(range(1,num_epochs))    \n",
    "plt.plot(xs, train_accs,label='Training Accuracy')\n",
    "plt.plot(xs, test_accs,label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c43177",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Max Testing Accuracy: {100*best_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a1dcb",
   "metadata": {},
   "source": [
    "### Looking at (in)correctly classified categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for data in test_loader:\n",
    "    \n",
    "    out = best_model(data.x,data.edge_index,data.batch)\n",
    "    y_pred = out.argmax(dim=1)\n",
    "    y_true = data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(np.array(y_true), np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from https://deeplizard.com/learn/video/0LhiS6yu2qQ\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(conf_mat, list(full_dataset['class_map'].values()), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12306e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model,'DRGNet1024_height.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d21f7",
   "metadata": {},
   "source": [
    "# Experiment 2: 1024 Points, Unnormalized, Width Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74cd3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"modelNetReebGraphs_1024_UnNormalized_width.pickle\", \"rb\") as f:\n",
    "    full_dataset_Reeb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea3b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = full_dataset_Reeb['train_data']\n",
    "testing_data = full_dataset_Reeb['test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "training_data_list = [convert_to_torch_format(training_data[j],training_labels[j]) for j in range(len(training_data))]\n",
    "train_loader = DataLoader(training_data_list, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "testing_data_list = [convert_to_torch_format(testing_data[j],testing_labels[j]) for j in range(len(testing_data))]\n",
    "test_loader = DataLoader(testing_data_list, batch_size = len(testing_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d33c3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CN3sRVuaQ88l",
    "outputId": "59c998ba-b387-4413-d7d3-2101a27495bd"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "node_feature_size = 35 # Need to change, depending on feature style\n",
    "num_data_classes = 10\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(node_feature_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_data_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5daea0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HvhgQoO8Svw4",
    "outputId": "4d17af35-85c8-4bcd-8e16-c514fd2ff714",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = GCN(hidden_channels=256)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "num_epochs = 300\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    test_acc = test(test_loader)\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_model = model\n",
    "    test_accs.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "xs = list(range(1,num_epochs))    \n",
    "plt.plot(xs, train_accs,label='Training Accuracy')\n",
    "plt.plot(xs, test_accs,label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Max Testing Accuracy: {100*best_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_loader:\n",
    "    out = best_model(data.x,data.edge_index,data.batch)\n",
    "    y_pred = out.argmax(dim=1)\n",
    "    y_true = data.y\n",
    "    \n",
    "conf_mat = confusion_matrix(np.array(y_true), np.array(y_pred))\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(conf_mat, list(full_dataset['class_map'].values()), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model,'DRGNet1024_width.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec77da29",
   "metadata": {},
   "source": [
    "## Combining Predictions for height and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b824eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"modelNetReebGraphs_1024_UnNormalized_height.pickle\", \"rb\") as f:\n",
    "    full_dataset_Reeb_height = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1057036",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"modelNetReebGraphs_1024_UnNormalized_width.pickle\", \"rb\") as f:\n",
    "    full_dataset_Reeb_width = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_list_height = [convert_to_torch_format(testing_data_height[j],testing_labels[j]) for j in range(len(testing_data_height))]\n",
    "test_loader_height = DataLoader(testing_data_list_height, batch_size = len(testing_data_height), shuffle=False)\n",
    "\n",
    "testing_data_list_width = [convert_to_torch_format(testing_data_width[j],testing_labels[j]) for j in range(len(testing_data_width))]\n",
    "test_loader_width = DataLoader(testing_data_list_width, batch_size = len(testing_data_width), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_model = torch.load('DRGNet1024_height.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b775354",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in height_model.parameters())\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e59459",
   "metadata": {},
   "outputs": [],
   "source": [
    "width_model = torch.load('DRGNet1024_width.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef613bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_loader_height:\n",
    "    out_height = height_model(data.x,data.edge_index,data.batch)\n",
    "    \n",
    "for data in test_loader_width:\n",
    "    out_width = width_model(data.x,data.edge_index,data.batch)\n",
    "    \n",
    "out = out_height + out_width\n",
    "y_pred = out.argmax(dim=1)\n",
    "y_true = data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Combined Prediction: {np.round(100*(sum(y_pred == y_true)/len(y_true)).item(),2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e284ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(np.array(y_true), np.array(y_pred))\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(conf_mat, list(full_dataset['class_map'].values()), normalize=True)\n",
    "plt.savefig('BestClassificationConfusionMatrix.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d2bf9",
   "metadata": {},
   "source": [
    "## Experiment 3: Combining Height and Width in the Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb898b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"modelNetReebGraphs_1024_UnNormalized_height.pickle\", \"rb\") as f:\n",
    "    full_dataset_Reeb_height = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"modelNetReebGraphs_1024_UnNormalized_width.pickle\", \"rb\") as f:\n",
    "    full_dataset_Reeb_width = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_height = full_dataset_Reeb_height['train_data']\n",
    "testing_data_height = full_dataset_Reeb_height['test_data']\n",
    "\n",
    "training_data_width = full_dataset_Reeb_width['train_data']\n",
    "testing_data_width = full_dataset_Reeb_width['test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761637ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_DRGs(G,H):\n",
    "    \n",
    "    Gnew = nx.convert_node_labels_to_integers(G)\n",
    "    Hnew = nx.convert_node_labels_to_integers(H,first_label = len(G))\n",
    "    GH = nx.union(Gnew,Hnew)\n",
    "    \n",
    "    return GH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [combine_DRGs(training_data_height[j],training_data_width[j]) for j in range(len(training_data_height))]\n",
    "testing_data = [combine_DRGs(testing_data_height[j],testing_data_width[j]) for j in range(len(testing_data_height))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a4e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "training_data_list = [convert_to_torch_format(training_data[j],training_labels[j]) for j in range(len(training_data))]\n",
    "train_loader = DataLoader(training_data_list, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "testing_data_list = [convert_to_torch_format(testing_data[j],testing_labels[j]) for j in range(len(testing_data))]\n",
    "test_loader = DataLoader(testing_data_list, batch_size = len(testing_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f161fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CN3sRVuaQ88l",
    "outputId": "59c998ba-b387-4413-d7d3-2101a27495bd"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "node_feature_size = 35 # Need to change, depending on feature style\n",
    "num_data_classes = 10\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(node_feature_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_data_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49381c00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HvhgQoO8Svw4",
    "outputId": "4d17af35-85c8-4bcd-8e16-c514fd2ff714",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = GCN(hidden_channels=256)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "num_epochs = 300\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    test_acc = test(test_loader)\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_model = model\n",
    "    test_accs.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "xs = list(range(1,num_epochs))    \n",
    "plt.plot(xs, train_accs,label='Training Accuracy')\n",
    "plt.plot(xs, test_accs,label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c53040",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Max Testing Accuracy: {100*best_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_loader:\n",
    "    out = best_model(data.x,data.edge_index,data.batch)\n",
    "    y_pred = out.argmax(dim=1)\n",
    "    y_true = data.y\n",
    "    \n",
    "conf_mat = confusion_matrix(np.array(y_true), np.array(y_pred))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(conf_mat, list(full_dataset['class_map'].values()), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78791380",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model,'DRGNet1024_height_and_width.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e9a8a",
   "metadata": {},
   "source": [
    "# Ablation: Trying without topological attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"modelNetReebGraphs_1024_UnNormalized_height.pickle\", \"rb\") as f:\n",
    "    full_dataset_Reeb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = full_dataset_Reeb['train_data']\n",
    "testing_data = full_dataset_Reeb['test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfc6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_feature_matrix_ablation(graph,attribute = 'persistence_statistics'):\n",
    "    \n",
    "    return np.array([[graph.nodes[node]['function value'],graph.nodes[node]['3d pos'][0],graph.nodes[node]['3d pos'][1],graph.nodes[node]['3d pos'][2],len(graph.nodes[node]['component indices'])] for node in graph.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch_format_ablation(graph,label):\n",
    "    \n",
    "    adjacency_matrix = nx.adjacency_matrix(graph)\n",
    "    edge_data = from_scipy_sparse_matrix(adjacency_matrix)\n",
    "    x = get_node_feature_matrix_ablation(graph)\n",
    "    y = label\n",
    "    \n",
    "    data = Data(x=torch.Tensor(x), edge_index=edge_data[0], edge_attr = edge_data[1], y=torch.tensor([y],dtype=torch.long))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "training_data_list = [convert_to_torch_format_ablation(training_data[j],training_labels[j]) for j in range(len(training_data))]\n",
    "train_loader = DataLoader(training_data_list, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "testing_data_list = [convert_to_torch_format_ablation(testing_data[j],testing_labels[j]) for j in range(len(testing_data))]\n",
    "test_loader = DataLoader(testing_data_list, batch_size = len(testing_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad507ad6",
   "metadata": {},
   "source": [
    "## Define Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca954a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CN3sRVuaQ88l",
    "outputId": "59c998ba-b387-4413-d7d3-2101a27495bd"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "node_feature_size = 5\n",
    "num_data_classes = 10\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(node_feature_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_data_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b92c5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HvhgQoO8Svw4",
    "outputId": "4d17af35-85c8-4bcd-8e16-c514fd2ff714",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=256)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "num_epochs = 200\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    test_acc = test(test_loader)\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_model = model\n",
    "    test_accs.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "xs = list(range(1,num_epochs))    \n",
    "plt.plot(xs, train_accs,label='Training Accuracy')\n",
    "plt.plot(xs, test_accs,label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aeadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Max Testing Accuracy: {100*best_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3100b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_loader:\n",
    "    out = best_model(data.x,data.edge_index,data.batch)\n",
    "    y_pred = out.argmax(dim=1)\n",
    "    y_true = data.y\n",
    "    \n",
    "conf_mat = confusion_matrix(np.array(y_true), np.array(y_pred))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(conf_mat, list(full_dataset['class_map'].values()), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model,'DRGNet1024_only_Reeb.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7841aef",
   "metadata": {},
   "source": [
    "# Ablation: Trying without Graph Structure (bag of  persistence diagrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705937d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_feature_matrix_ablation2(graph,attribute = 'persistence_statistics'):\n",
    "    \n",
    "    return np.array([graph.nodes[node][attribute] for node in graph.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch_format_ablation2(graph,label):\n",
    "    \n",
    "    adjacency_matrix = nx.adjacency_matrix(nx.complete_graph(len(graph)))\n",
    "    edge_data = from_scipy_sparse_matrix(adjacency_matrix)\n",
    "    x = get_node_feature_matrix_ablation2(graph)\n",
    "    y = label\n",
    "    \n",
    "    data = Data(x=torch.Tensor(x), edge_index=edge_data[0], edge_attr = edge_data[1], y=torch.tensor([y],dtype=torch.long))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "training_data_list = [convert_to_torch_format_ablation2(training_data[j],training_labels[j]) for j in range(len(training_data))]\n",
    "train_loader = DataLoader(training_data_list, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "testing_data_list = [convert_to_torch_format_ablation2(testing_data[j],testing_labels[j]) for j in range(len(testing_data))]\n",
    "test_loader = DataLoader(testing_data_list, batch_size = len(testing_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdc326",
   "metadata": {},
   "source": [
    "## Define Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf961c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CN3sRVuaQ88l",
    "outputId": "59c998ba-b387-4413-d7d3-2101a27495bd"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "node_feature_size = 30\n",
    "num_data_classes = 10\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(node_feature_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_data_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274ec14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HvhgQoO8Svw4",
    "outputId": "4d17af35-85c8-4bcd-8e16-c514fd2ff714",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=256)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "num_epochs = 200\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    test_acc = test(test_loader)\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_model = model\n",
    "    test_accs.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "xs = list(range(1,num_epochs))    \n",
    "plt.plot(xs, train_accs,label='Training Accuracy')\n",
    "plt.plot(xs, test_accs,label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfdd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Max Testing Accuracy: {100*best_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a54690",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_loader:\n",
    "    out = best_model(data.x,data.edge_index,data.batch)\n",
    "    y_pred = out.argmax(dim=1)\n",
    "    y_true = data.y\n",
    "    \n",
    "conf_mat = confusion_matrix(np.array(y_true), np.array(y_pred))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(conf_mat, list(full_dataset['class_map'].values()), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model,'DRGNet1024_only_Diagrams.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drgGNN",
   "language": "python",
   "name": "drggnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
